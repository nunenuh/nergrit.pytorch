{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "# sys.path.append('../')\n",
    "# os.chdir('../')\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertConfig, BertTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nergrit.word_classification import BertForWordClassification\n",
    "from nergrit.forward_fn import forward_word_classification\n",
    "from nergrit.metrics import ner_metrics_fn\n",
    "from nergrit.data import NerKtpDataset, NerDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# common functions\n",
    "###\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "def count_param(module, trainable=False):\n",
    "    if trainable:\n",
    "        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return sum(p.numel() for p in module.parameters())\n",
    "    \n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def metrics_to_string(metric_dict):\n",
    "    string_list = []\n",
    "    for key, value in metric_dict.items():\n",
    "        string_list.append('{}:{:.2f}'.format(key, value))\n",
    "    return ' '.join(string_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "set_seed(26092020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load IndoBert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForWordClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Tokenizer and Config\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p2')\n",
    "config = BertConfig.from_pretrained('indobenchmark/indobert-base-p2')\n",
    "config.num_labels = NerKtpDataset.NUM_LABELS\n",
    "\n",
    "# Instantiate model\n",
    "model = BertForWordClassification.from_pretrained('indobenchmark/indobert-base-p2', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForWordClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(50000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=69, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124494405"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_param(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Named Entity Recognition Dataset (NERGrit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = 'data/idcard/ktp_ner_train_dataset.csv'\n",
    "valid_dataset_path = 'data/idcard/ktp_ner_valid_dataset.csv'\n",
    "test_dataset_path = 'data/idcard/ktp_ner_test_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8250/8250 [00:42<00:00, 192.20it/s]\n",
      "100%|██████████| 1788/1788 [00:08<00:00, 198.84it/s]\n",
      "100%|██████████| 962/962 [00:04<00:00, 199.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.1 s, sys: 150 ms, total: 57.2 s\n",
      "Wall time: 56.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_dataset = NerKtpDataset(train_dataset_path, tokenizer, lowercase=False)\n",
    "valid_dataset = NerKtpDataset(valid_dataset_path, tokenizer, lowercase=False)\n",
    "test_dataset = NerKtpDataset(test_dataset_path, tokenizer, lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = NerDataLoader(dataset=train_dataset, max_seq_len=512, batch_size=8, num_workers=16, shuffle=True)  \n",
    "valid_loader = NerDataLoader(dataset=valid_dataset, max_seq_len=512, batch_size=8, num_workers=16, shuffle=False)  \n",
    "test_loader = NerDataLoader(dataset=test_dataset, max_seq_len=512, batch_size=8, num_workers=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'U-FLD_PROV': 0, 'B-VAL_PROV': 1, 'L-VAL_PROV': 2, 'U-FLD_KAB': 3, 'U-VAL_KAB': 4, 'U-FLD_NIK': 5, 'U-VAL_NIK': 6, 'U-FLD_NAMA': 7, 'B-VAL_NAMA': 8, 'L-VAL_NAMA': 9, 'B-FLD_TTL': 10, 'L-FLD_TTL': 11, 'B-VAL_TTL': 12, 'L-VAL_TTL': 13, 'B-FLD_GDR': 14, 'L-FLD_GDR': 15, 'U-VAL_GDR': 16, 'B-FLD_GLD': 17, 'L-FLD_GLD': 18, 'U-VAL_GLD': 19, 'U-FLD_ADR': 20, 'B-VAL_ADR': 21, 'I-VAL_ADR': 22, 'L-VAL_ADR': 23, 'U-FLD_RTW': 24, 'U-VAL_RTW': 25, 'U-FLD_KLH': 26, 'U-VAL_KLH': 27, 'U-FLD_KCM': 28, 'U-VAL_KCM': 29, 'U-FLD_RLG': 30, 'U-VAL_RLG': 31, 'B-FLD_KWN': 32, 'L-FLD_KWN': 33, 'B-VAL_KWN': 34, 'L-VAL_KWN': 35, 'U-FLD_KRJ': 36, 'U-VAL_KRJ': 37, 'U-FLD_WRG': 38, 'U-VAL_WRG': 39, 'B-FLD_BLK': 40, 'L-FLD_BLK': 41, 'B-VAL_BLK': 42, 'L-VAL_BLK': 43, 'U-VAL_SGP': 44, 'U-VAL_SGD': 45, 'B-VAL_KAB': 46, 'L-VAL_KAB': 47, 'U-VAL_NAMA': 48, 'B-VAL_KLH': 49, 'L-VAL_KLH': 50, 'B-VAL_KRJ': 51, 'I-VAL_KRJ': 52, 'L-VAL_KRJ': 53, 'B-VAL_SGP': 54, 'L-VAL_SGP': 55, 'I-VAL_TTL': 56, 'L-VAL_KCM': 57, 'B-VAL_KCM': 58, 'U-VAL_KWN': 59, 'U-VAL_PROV': 60, 'I-VAL_NAMA': 61, 'I-VAL_PROV': 62, 'I-VAL_KAB': 63, 'I-VAL_KCM': 64, 'I-VAL_SGP': 65, 'U-VAL_ADR': 66, 'I-VAL_KLH': 67, 'O': 68}\n",
      "{0: 'U-FLD_PROV', 1: 'B-VAL_PROV', 2: 'L-VAL_PROV', 3: 'U-FLD_KAB', 4: 'U-VAL_KAB', 5: 'U-FLD_NIK', 6: 'U-VAL_NIK', 7: 'U-FLD_NAMA', 8: 'B-VAL_NAMA', 9: 'L-VAL_NAMA', 10: 'B-FLD_TTL', 11: 'L-FLD_TTL', 12: 'B-VAL_TTL', 13: 'L-VAL_TTL', 14: 'B-FLD_GDR', 15: 'L-FLD_GDR', 16: 'U-VAL_GDR', 17: 'B-FLD_GLD', 18: 'L-FLD_GLD', 19: 'U-VAL_GLD', 20: 'U-FLD_ADR', 21: 'B-VAL_ADR', 22: 'I-VAL_ADR', 23: 'L-VAL_ADR', 24: 'U-FLD_RTW', 25: 'U-VAL_RTW', 26: 'U-FLD_KLH', 27: 'U-VAL_KLH', 28: 'U-FLD_KCM', 29: 'U-VAL_KCM', 30: 'U-FLD_RLG', 31: 'U-VAL_RLG', 32: 'B-FLD_KWN', 33: 'L-FLD_KWN', 34: 'B-VAL_KWN', 35: 'L-VAL_KWN', 36: 'U-FLD_KRJ', 37: 'U-VAL_KRJ', 38: 'U-FLD_WRG', 39: 'U-VAL_WRG', 40: 'B-FLD_BLK', 41: 'L-FLD_BLK', 42: 'B-VAL_BLK', 43: 'L-VAL_BLK', 44: 'U-VAL_SGP', 45: 'U-VAL_SGD', 46: 'B-VAL_KAB', 47: 'L-VAL_KAB', 48: 'U-VAL_NAMA', 49: 'B-VAL_KLH', 50: 'L-VAL_KLH', 51: 'B-VAL_KRJ', 52: 'I-VAL_KRJ', 53: 'L-VAL_KRJ', 54: 'B-VAL_SGP', 55: 'L-VAL_SGP', 56: 'I-VAL_TTL', 57: 'L-VAL_KCM', 58: 'B-VAL_KCM', 59: 'U-VAL_KWN', 60: 'U-VAL_PROV', 61: 'I-VAL_NAMA', 62: 'I-VAL_PROV', 63: 'I-VAL_KAB', 64: 'I-VAL_KCM', 65: 'I-VAL_SGP', 66: 'U-VAL_ADR', 67: 'I-VAL_KLH', 68: 'O'}\n"
     ]
    }
   ],
   "source": [
    "w2i, i2w = NerKtpDataset.LABEL2INDEX, NerKtpDataset.INDEX2LABEL\n",
    "print(w2i)\n",
    "print(i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    1,    2,    3,    4,    5,    6,    7,    8,   61,    9,\n",
       "          10,   11,   12,   13,   14,   15,   16,   17,   18,   19,   20,\n",
       "          21,   22,   22,   22,   23,   24,   25,   26,   27,   28,   29,\n",
       "          30,   31,   32,   33,   59,   36,   37,   38,   39,   40,   41,\n",
       "          42,   43,   44,   45, -100, -100, -100, -100, -100],\n",
       "       [   0,    1,    2,    3,    4,    5,    6,    7,    8,   61,    9,\n",
       "          10,   11,   12,   13,   14,   15,   16,   17,   18,   19,   20,\n",
       "          21,   22,   22,   22,   23,   24,   25,   26,   27,   28,   29,\n",
       "          30,   31,   32,   33,   59,   36,   51,   53,   38,   39,   40,\n",
       "          41,   42,   43,   44,   45, -100, -100, -100, -100],\n",
       "       [   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,   10,\n",
       "          11,   12,   13,   14,   15,   16,   17,   18,   19,   20,   21,\n",
       "          23,   24,   25,   26,   27,   28,   29,   30,   31,   32,   33,\n",
       "          59,   36,   51,   53,   38,   39,   40,   41,   42,   43,   44,\n",
       "          45, -100, -100, -100, -100, -100, -100, -100, -100],\n",
       "       [   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,   10,\n",
       "          11,   12,   13,   14,   15,   16,   17,   18,   19,   20,   21,\n",
       "          22,   22,   22,   22,   22,   22,   23,   24,   25,   26,   49,\n",
       "          50,   28,   29,   30,   31,   32,   33,   34,   35,   36,   51,\n",
       "          53,   38,   39,   40,   41,   42,   43,   44,   45],\n",
       "       [  32,   33,   59,   20,   21,   22,   22,   22,   22,   22,   23,\n",
       "          14,   15,   16,   44,   30,   31,   17,   18,   19,   28,   29,\n",
       "           0,    1,    2,   26,   49,   50,    3,    4,   45,   38,   39,\n",
       "          36,   37,   24,   25,    5,    6,    7,    8,    9,   40,   41,\n",
       "          42,   43,   10,   11,   12,   13, -100, -100, -100],\n",
       "       [   0,    1,    2,    3,    4,    5,    6,    7,    8,   61,    9,\n",
       "          10,   11,   12,   13,   14,   15,   16,   17,   18,   19,   20,\n",
       "          21,   22,   22,   23,   24,   25,   26,   27,   28,   29,   30,\n",
       "          31,   32,   33,   59,   36,   51,   53,   38,   39,   40,   41,\n",
       "          42,   43,   44,   45, -100, -100, -100, -100, -100],\n",
       "       [   0,   60,    3,    4,    5,    6,    7,    8,   61,    9,   10,\n",
       "          11,   12,   13,   14,   15,   16,   17,   18,   19,   20,   21,\n",
       "          22,   22,   22,   22,   23,   24,   25,   26,   27,   28,   29,\n",
       "          30,   31,   32,   33,   34,   35,   36,   37,   38,   39,   40,\n",
       "          41,   42,   43,   44,   45, -100, -100, -100, -100],\n",
       "       [   0,    1,   62,   62,   62,    2,    3,    4,    5,    6,    7,\n",
       "           8,    9,   10,   11,   12,   13,   14,   15,   16,   17,   18,\n",
       "          19,   20,   21,   22,   22,   22,   22,   23,   24,   25,   26,\n",
       "          27,   28,   29,   30,   31,   32,   33,   34,   35,   36,   37,\n",
       "          38,   39,   40,   41,   42,   43,   44,   45, -100]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "subword_batch, mask_batch, subword_to_word_indices_batch, seq_label_batch, seq_list = batch\n",
    "seq_label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model on sample sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_subword_tokenize(sentence, tokenizer):\n",
    "    # Add CLS token\n",
    "    subwords = [tokenizer.cls_token_id]\n",
    "    subword_to_word_indices = [-1] # For CLS\n",
    "\n",
    "    # Add subwords\n",
    "    for word_idx, word in enumerate(sentence):\n",
    "        subword_list = tokenizer.encode(word, add_special_tokens=False)\n",
    "        subword_to_word_indices += [word_idx for i in range(len(subword_list))]\n",
    "        subwords += subword_list\n",
    "\n",
    "    # Add last SEP token\n",
    "    subwords += [tokenizer.sep_token_id]\n",
    "    subword_to_word_indices += [-1]\n",
    "\n",
    "    return subwords, subword_to_word_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-ca4f45d29dd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubword_to_word_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi2w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "text = word_tokenize('Nama Lalu Erfandi Maula Yusnu')\n",
    "subwords, subword_to_word_indices = word_subword_tokenize(text, tokenizer)\n",
    "\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "subword_to_word_indices = torch.LongTensor(subword_to_word_indices).view(1, -1).to(model.device)\n",
    "logits = model(subwords, subword_to_word_indices)[0]\n",
    "\n",
    "preds = torch.topk(logits, k=1, dim=-1)[1].squeeze().numpy()\n",
    "labels = [i2w[preds[i]] for i in range(len(preds))]\n",
    "\n",
    "pd.DataFrame({'words': text, 'label': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alamat</td>\n",
       "      <td>U-FLD_ADR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JL</td>\n",
       "      <td>B-VAL_ADR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MUSIUM</td>\n",
       "      <td>I-VAL_ADR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NO</td>\n",
       "      <td>I-VAL_ADR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>I-VAL_ADR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A</td>\n",
       "      <td>L-VAL_ADR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    words      label\n",
       "0  Alamat  U-FLD_ADR\n",
       "1      JL  B-VAL_ADR\n",
       "2  MUSIUM  I-VAL_ADR\n",
       "3      NO  I-VAL_ADR\n",
       "4      19  I-VAL_ADR\n",
       "5       A  L-VAL_ADR"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = word_tokenize('Alamat JL MUSIUM NO 19 A')\n",
    "subwords, subword_to_word_indices = word_subword_tokenize(text, tokenizer)\n",
    "\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "subword_to_word_indices = torch.LongTensor(subword_to_word_indices).view(1, -1).to(model.device)\n",
    "logits = model(subwords, subword_to_word_indices)[0]\n",
    "\n",
    "preds = torch.topk(logits, k=1, dim=-1)[1].squeeze().cpu().numpy()\n",
    "labels = [i2w[preds[i]] for i in range(len(preds))]\n",
    "\n",
    "pd.DataFrame({'words': text, 'label': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PROVINSI</td>\n",
       "      <td>U-FLD_PROV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NUSA</td>\n",
       "      <td>B-VAL_PROV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TENGGARA</td>\n",
       "      <td>I-VAL_PROV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BARAT</td>\n",
       "      <td>L-VAL_PROV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      words       label\n",
       "0  PROVINSI  U-FLD_PROV\n",
       "1      NUSA  B-VAL_PROV\n",
       "2  TENGGARA  I-VAL_PROV\n",
       "3     BARAT  L-VAL_PROV"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = word_tokenize('PROVINSI NUSA TENGGARA BARAT')\n",
    "subwords, subword_to_word_indices = word_subword_tokenize(text, tokenizer)\n",
    "\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "subword_to_word_indices = torch.LongTensor(subword_to_word_indices).view(1, -1).to(model.device)\n",
    "logits = model(subwords, subword_to_word_indices)[0]\n",
    "\n",
    "preds = torch.topk(logits, k=1, dim=-1)[1].squeeze().cpu().numpy()\n",
    "labels = [i2w[preds[i]] for i in range(len(preds))]\n",
    "\n",
    "pd.DataFrame({'words': text, 'label': labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.0663 LR:0.00002000: 100%|██████████| 1032/1032 [01:52<00:00,  9.15it/s]\n",
      "  0%|          | 0/224 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.0663 ACC:0.99 F1:0.98 REC:0.98 PRE:0.98 LR:0.00002000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1019 ACC:0.99 F1:0.98 REC:0.98 PRE:0.98: 100%|██████████| 224/224 [00:39<00:00,  5.65it/s]\n",
      "  0%|          | 0/1032 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.1019 ACC:0.99 F1:0.98 REC:0.98 PRE:0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.0482 LR:0.00002000: 100%|██████████| 1032/1032 [01:52<00:00,  9.15it/s]\n",
      "  0%|          | 0/224 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.0482 ACC:0.99 F1:0.99 REC:0.99 PRE:0.99 LR:0.00002000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.0935 ACC:0.99 F1:0.98 REC:0.98 PRE:0.98: 100%|██████████| 224/224 [00:39<00:00,  5.65it/s]\n",
      "  0%|          | 0/1032 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.0935 ACC:0.99 F1:0.98 REC:0.98 PRE:0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.0374 LR:0.00002000: 100%|██████████| 1032/1032 [01:52<00:00,  9.17it/s]\n",
      "  0%|          | 0/224 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.0374 ACC:1.00 F1:0.99 REC:0.99 PRE:0.99 LR:0.00002000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.0972 ACC:0.99 F1:0.98 REC:0.98 PRE:0.98: 100%|██████████| 224/224 [00:39<00:00,  5.64it/s]\n",
      "  0%|          | 0/1032 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.0972 ACC:0.99 F1:0.98 REC:0.98 PRE:0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.0311 LR:0.00002000: 100%|██████████| 1032/1032 [01:53<00:00,  9.11it/s]\n",
      "  0%|          | 0/224 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.0311 ACC:1.00 F1:0.99 REC:0.99 PRE:0.99 LR:0.00002000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.0933 ACC:0.99 F1:0.98 REC:0.98 PRE:0.98: 100%|██████████| 224/224 [00:39<00:00,  5.62it/s]\n",
      "  0%|          | 0/1032 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.0933 ACC:0.99 F1:0.98 REC:0.98 PRE:0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.0256 LR:0.00002000: 100%|██████████| 1032/1032 [01:52<00:00,  9.16it/s]\n",
      "  0%|          | 0/224 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.0256 ACC:1.00 F1:0.99 REC:0.99 PRE:0.99 LR:0.00002000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.0916 ACC:0.99 F1:0.98 REC:0.98 PRE:0.98: 100%|██████████| 224/224 [00:39<00:00,  5.64it/s]\n",
      "  0%|          | 0/1032 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.0916 ACC:0.99 F1:0.98 REC:0.98 PRE:0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.0209 LR:0.00002000: 100%|██████████| 1032/1032 [01:53<00:00,  9.12it/s]\n",
      "  0%|          | 0/224 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.0209 ACC:1.00 F1:0.99 REC:0.99 PRE:0.99 LR:0.00002000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.0872 ACC:0.99 F1:0.98 REC:0.98 PRE:0.98: 100%|██████████| 224/224 [00:39<00:00,  5.66it/s]\n",
      "  0%|          | 0/1032 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) VALID LOSS:0.0872 ACC:0.99 F1:0.98 REC:0.98 PRE:0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.0185 LR:0.00002000: 100%|██████████| 1032/1032 [01:53<00:00,  9.12it/s]\n",
      "  0%|          | 0/224 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.0185 ACC:1.00 F1:0.99 REC:0.99 PRE:0.99 LR:0.00002000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.0896 ACC:0.99 F1:0.98 REC:0.98 PRE:0.98: 100%|██████████| 224/224 [00:39<00:00,  5.65it/s]\n",
      "  0%|          | 0/1032 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) VALID LOSS:0.0896 ACC:0.99 F1:0.98 REC:0.98 PRE:0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.0168 LR:0.00002000:  57%|█████▋    | 584/1032 [01:04<00:48,  9.28it/s]"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "n_epochs = 8\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    " \n",
    "    total_train_loss = 0\n",
    "    list_hyp, list_label = [], []\n",
    "\n",
    "    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\n",
    "    for i, batch_data in enumerate(train_pbar):\n",
    "        # Forward model\n",
    "        loss, batch_hyp, batch_label = forward_word_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "\n",
    "        # Update model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss = loss.item()\n",
    "        total_train_loss = total_train_loss + tr_loss\n",
    "\n",
    "        # Calculate metrics\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "\n",
    "        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\n",
    "            total_train_loss/(i+1), get_lr(optimizer)))\n",
    "\n",
    "    # Calculate train metric\n",
    "    metrics = ner_metrics_fn(list_hyp, list_label)\n",
    "    print(\"(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}\".format((epoch+1),\n",
    "        total_train_loss/(i+1), metrics_to_string(metrics), get_lr(optimizer)))\n",
    "\n",
    "    # Evaluate on validation\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    \n",
    "    total_loss, total_correct, total_labels = 0, 0, 0\n",
    "    list_hyp, list_label = [], []\n",
    "\n",
    "    pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\n",
    "    for i, batch_data in enumerate(pbar):\n",
    "        batch_seq = batch_data[-1]        \n",
    "        loss, batch_hyp, batch_label = forward_word_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "        \n",
    "        # Calculate total loss\n",
    "        valid_loss = loss.item()\n",
    "        total_loss = total_loss + valid_loss\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "        metrics = ner_metrics_fn(list_hyp, list_label)\n",
    "\n",
    "        pbar.set_description(\"VALID LOSS:{:.4f} {}\".format(total_loss/(i+1), metrics_to_string(metrics)))\n",
    "        \n",
    "    metrics = ner_metrics_fn(list_hyp, list_label)\n",
    "    print(\"(Epoch {}) VALID LOSS:{:.4f} {}\".format((epoch+1),\n",
    "        total_loss/(i+1), metrics_to_string(metrics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "total_loss, total_correct, total_labels = 0, 0, 0\n",
    "list_hyp, list_label = [], []\n",
    "\n",
    "pbar = tqdm(test_loader, leave=True, total=len(test_loader))\n",
    "for i, batch_data in enumerate(pbar):\n",
    "    _, batch_hyp, _ = forward_word_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "    list_hyp += batch_hyp\n",
    "\n",
    "# Save prediction\n",
    "df = pd.DataFrame({'label':list_hyp}).reset_index()\n",
    "df.to_csv('pred.txt', index=False)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test fine-tuned model with sample sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = word_tokenize('Jl Jendral Sudirman No 15')\n",
    "subwords, subword_to_word_indices = word_subword_tokenize(text, tokenizer)\n",
    "\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "subword_to_word_indices = torch.LongTensor(subword_to_word_indices).view(1, -1).to(model.device)\n",
    "logits = model(subwords, subword_to_word_indices)[0]\n",
    "\n",
    "preds = torch.topk(logits, k=1, dim=-1)[1].squeeze().cpu().numpy()\n",
    "labels = [i2w[preds[i]] for i in range(len(preds))]\n",
    "\n",
    "pd.DataFrame({'words': text, 'label': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = word_tokenize('Budi pergi ke mall kelapa gading membeli kue bantal')\n",
    "subwords, subword_to_word_indices = word_subword_tokenize(text, tokenizer)\n",
    "\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "subword_to_word_indices = torch.LongTensor(subword_to_word_indices).view(1, -1).to(model.device)\n",
    "logits = model(subwords, subword_to_word_indices)[0]\n",
    "\n",
    "preds = torch.topk(logits, k=1, dim=-1)[1].squeeze().cpu().numpy()\n",
    "labels = [i2w[preds[i]] for i in range(len(preds))]\n",
    "\n",
    "pd.DataFrame({'words': text, 'label': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = word_tokenize('Saya sudah sampai di depan menara bca')\n",
    "subwords, subword_to_word_indices = word_subword_tokenize(text, tokenizer)\n",
    "\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "subword_to_word_indices = torch.LongTensor(subword_to_word_indices).view(1, -1).to(model.device)\n",
    "logits = model(subwords, subword_to_word_indices)[0]\n",
    "\n",
    "preds = torch.topk(logits, k=1, dim=-1)[1].squeeze().cpu().numpy()\n",
    "labels = [i2w[preds[i]] for i in range(len(preds))]\n",
    "\n",
    "pd.DataFrame({'words': text, 'label': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
