{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import torch\n",
    "import re\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from transformers import BertConfig, BertTokenizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerIDCardDataset(Dataset):\n",
    "    # Static constant variable\n",
    "    LABELS = [\n",
    "        'U-FLD_PROV', 'B-VAL_PROV', 'L-VAL_PROV', 'U-FLD_KAB', 'U-VAL_KAB',\n",
    "        'U-FLD_NIK', 'U-VAL_NIK', 'U-FLD_NAMA', 'B-VAL_NAMA', 'L-VAL_NAMA',\n",
    "        'B-FLD_TTL', 'L-FLD_TTL', 'B-VAL_TTL', 'L-VAL_TTL', 'B-FLD_GDR',\n",
    "        'L-FLD_GDR', 'U-VAL_GDR', 'B-FLD_GLD', 'L-FLD_GLD', 'U-VAL_GLD',\n",
    "        'U-FLD_ADR', 'B-VAL_ADR', 'I-VAL_ADR', 'L-VAL_ADR', 'U-FLD_RTW',\n",
    "        'U-VAL_RTW', 'U-FLD_KLH', 'U-VAL_KLH', 'U-FLD_KCM', 'U-VAL_KCM',\n",
    "        'U-FLD_RLG', 'U-VAL_RLG', 'B-FLD_KWN', 'L-FLD_KWN', 'B-VAL_KWN',\n",
    "        'L-VAL_KWN', 'U-FLD_KRJ', 'U-VAL_KRJ', 'U-FLD_WRG', 'U-VAL_WRG',\n",
    "        'B-FLD_BLK', 'L-FLD_BLK', 'B-VAL_BLK', 'L-VAL_BLK', 'U-VAL_SGP',\n",
    "        'U-VAL_SGD', 'B-VAL_KAB', 'L-VAL_KAB', 'U-VAL_NAMA', 'B-VAL_KLH',\n",
    "        'L-VAL_KLH', 'B-VAL_KRJ', 'I-VAL_KRJ', 'L-VAL_KRJ', 'B-VAL_SGP',\n",
    "        'L-VAL_SGP', 'I-VAL_TTL', 'L-VAL_KCM', 'B-VAL_KCM', 'U-VAL_KWN',\n",
    "        'U-VAL_PROV', 'I-VAL_NAMA', 'I-VAL_PROV', 'I-VAL_KAB', 'I-VAL_KCM',\n",
    "        'I-VAL_SGP', 'U-VAL_ADR', 'I-VAL_KLH', 'O'\n",
    "    ]\n",
    "    \n",
    "    LABEL2INDEX = dict((label,idx) for idx, label in enumerate(LABELS))\n",
    "    INDEX2LABEL = dict((idx, label) for idx, label in enumerate(LABELS))\n",
    "    NUM_LABELS = len(LABELS)\n",
    "    \n",
    "    \n",
    "    def __init__(self, dataset_path, tokenizer, *args, **kwargs):\n",
    "        self.data = self.load_dataset(dataset_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def load_dataset(self, path):\n",
    "        dframe = pd.read_csv(path)\n",
    "        \n",
    "        dataset, sentence, seq_label = [], [], []\n",
    "        length_sentence = len(dframe.sentence_idx.unique())\n",
    "        for idx in range(length_sentence):\n",
    "            sframe = dframe[dframe.sentence_idx == idx]\n",
    "            for sidx in range(len(sframe)):\n",
    "                line = sframe.iloc[sidx]\n",
    "                word = str(line.word)\n",
    "                label = str(line.tag)\n",
    "                sentence.append(word)\n",
    "                seq_label.append(self.LABEL2INDEX[label])\n",
    "            dataset.append({\n",
    "                'sentence': sentence,\n",
    "                'seq_label': seq_label\n",
    "            })\n",
    "            sentence, seq_label = [], []\n",
    "        return dataset\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        sentence, seq_label = data['sentence'], data['seq_label']\n",
    "        \n",
    "        # Add CLS token\n",
    "        subwords = [self.tokenizer.cls_token_id]\n",
    "        subword_to_word_indices = [-1] # For CLS\n",
    "        \n",
    "        # Add subwords\n",
    "        for word_idx, word in enumerate(sentence):\n",
    "            subword_list = self.tokenizer.encode(word, add_special_tokens=False)\n",
    "            subword_to_word_indices += [word_idx for i in range(len(subword_list))]\n",
    "            subwords += subword_list\n",
    "            \n",
    "        # Add last SEP token\n",
    "        subwords += [self.tokenizer.sep_token_id]\n",
    "        subword_to_word_indices += [-1]\n",
    "        \n",
    "        return np.array(subwords), np.array(subword_to_word_indices), np.array(seq_label), data['sentence']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) \n",
    "        \n",
    "class NerDataLoader(DataLoader):\n",
    "    def __init__(self, max_seq_len=512, *args, **kwargs):\n",
    "        super(NerDataLoader, self).__init__(*args, **kwargs)\n",
    "        self.collate_fn = self._collate_fn\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def _collate_fn(self, batch):\n",
    "        batch_size = len(batch)\n",
    "        max_seq_len = max(map(lambda x: len(x[0]), batch))\n",
    "        max_seq_len = min(self.max_seq_len, max_seq_len)\n",
    "        max_tgt_len = max(map(lambda x: len(x[2]), batch))\n",
    "        \n",
    "        subword_batch = np.zeros((batch_size, max_seq_len), dtype=np.int64)\n",
    "        mask_batch = np.zeros((batch_size, max_seq_len), dtype=np.float32)\n",
    "        subword_to_word_indices_batch = np.full((batch_size, max_seq_len), -1, dtype=np.int64)\n",
    "        seq_label_batch = np.full((batch_size, max_tgt_len), -100, dtype=np.int64)\n",
    "        \n",
    "        seq_list = []\n",
    "        for i, (subwords, subword_to_word_indices, seq_label, raw_seq) in enumerate(batch):\n",
    "            subwords = subwords[:max_seq_len]\n",
    "            subword_to_word_indices = subword_to_word_indices[:max_seq_len]\n",
    "            \n",
    "            subword_batch[i,:len(subwords)] = subwords\n",
    "            mask_batch[i,:len(subwords)] = 1\n",
    "            subword_to_word_indices_batch[i,:len(subwords)] = subword_to_word_indices\n",
    "            seq_label_batch[i,:len(seq_label)] = seq_label\n",
    "\n",
    "            seq_list.append(raw_seq)\n",
    "            \n",
    "        return subword_batch, mask_batch, subword_to_word_indices_batch, seq_label_batch, seq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'data/idcard/ktp_ner_dataset.csv'\n",
    "pretrained_model = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "trainset = NerIDCardDataset(dataset_path, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NerIDCardDataset.LABEL2INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  101  4013  6371  5332  5730  2050  5199  3126 10556  8569 17585  2078\n",
      "  8945 14339 20265  3217 23205 28906 17465 21057 12740  2575  2683 21057\n",
      " 14142  2629 15125  2050  9152  3490  6520 22734  3736 18780  2527  8915\n",
      "  8737  4017  1013  1056 23296  2474 11961  8945 14339 20265  3217  1010\n",
      "  5840  1011  5757  1011  2639 15419  2483 17710 10278  2378 23976  8737\n",
      " 13860  2175  2140  1012 18243  4430  1051 26234  4017  1046  2140  3103\n",
      "  3334  9152  2099 21761  2004  3089 12849  8737  2140  9152  2099 21761\n",
      "  3103  3334  9097  1038  2140  1037  1013 10715 19387  1013  1054  2860\n",
      "  5890  2549  1013  5890  2487 17710  2140  1013  4078  2050 17710 11735\n",
      "  8490  5575 17710 28727  6790  2078 11687 18222  2078 12943  8067  7025\n",
      "  3570  2566  2912 10105  2319  8292 14995 13523  2072 21877  5484  3900\n",
      "  2319  9004  7088  1013 21877  3489  8569  2078 17710  9028  5289 29107\n",
      "  2527  2319  1059  3490  2022 12190  4817  2226  7632  3070  3654  7367\n",
      "  2819  3126 11041  6279  8945 14339 20265  3217  5890  1011  2260  1011\n",
      "  2325   102] 170\n",
      "[-1  0  0  0  1  1  2  2  3  3  3  3  4  4  4  4  5  6  6  6  6  6  6  6\n",
      "  6  6  7  7  8  8  9  9  9  9  9 10 10 10 10 10 10 11 11 12 12 12 12 12\n",
      " 13 13 13 13 13 14 14 15 15 15 16 16 16 17 17 17 18 18 19 20 20 21 21 22\n",
      " 22 23 23 23 24 24 25 25 25 26 26 26 27 27 28 29 29 30 30 30 31 31 31 31\n",
      " 32 32 32 32 32 33 33 33 33 33 34 34 34 34 35 35 35 35 36 36 36 37 37 38\n",
      " 39 40 40 40 40 41 41 42 42 43 43 43 43 44 44 44 44 44 44 44 45 45 45 45\n",
      " 45 45 46 46 47 47 47 47 48 48 48 49 49 49 50 50 51 51 51 51 52 52 52 52\n",
      " 52 -1] 170\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 22\n",
      " 22 22 22 22 22 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40\n",
      " 41 42 43 44 45] 53\n",
      "['PROVINSI', 'JAWA', 'TIMUR', 'KABUPATEN', 'BOJONEGORO', 'NIK', '3522190406990305', 'Nama', 'NINI', 'DJAJASAPUTRA', 'Tempat/Tgl', 'Lahir', 'BOJONEGORO,', '04-06-1999', 'Jenis', 'kelamin', 'PEREMPUAN', 'Gol.', 'Darah', 'O', 'Alamat', 'JL', 'SUNTER', 'NIRWANA', 'ASRI', 'KOMPL', 'NIRWANA', 'SUNTER', 'PARADISE', 'BL', 'A/108', 'RT/RW', '014/011', 'Kel/Desa', 'KEBONAGUNG', 'Kecamatan', 'PADANGAN', 'Agama', 'ISLAM', 'Status', 'Perkawinan', 'CERAI', 'MATI', 'Pekerjaan', 'PETANI/PEKEBUN', 'Kewarganegaraan', 'WNI', 'Berlaku', 'Hingga', 'SEUMUR', 'HIDUP', 'BOJONEGORO', '01-12-2015'] 53\n"
     ]
    }
   ],
   "source": [
    "for subwords, subword_to_word_indices, seq_label, sentence in trainset:\n",
    "    print(subwords, len(subwords))\n",
    "    print(subword_to_word_indices, len(subword_to_word_indices))\n",
    "    print(seq_label, len(seq_label))\n",
    "    print(sentence, len(sentence))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = NerDataLoader(dataset=trainset, batch_size=32, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for i, (subwords, mask, subword_to_word_indices, seq_label, seq_list) in enumerate(loader):\n",
    "#     print(subwords, mask, subword_to_word_indices, seq_label, seq_list)\n",
    "#     if i == 2:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
